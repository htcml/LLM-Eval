{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Multi-armed bandit (MAB) experiments aim to maximize performance of the primary metric across all variants. They achieve this by acquiring information about each variant (exploration) while simultaneously using updated information to optimize immediate rewards (exploitation). The latter is done by moving traffic gradually towards variants that are performing well while allocating less traffic to variants that are underperforming.\n",
    "\n",
    "## Thompson Sampling\n",
    "\n",
    "There are many MAB algorithms. Examples are Upper Confidence Bound, Greedy and Thompson Sampling. According to research studies[1], Thompson Sampling is one of the best in terms of reward maximization and currently used as the MAB algorithm in XP. Thompson sampling uses the Bayesian approach. It draw samples from posterior distribution and assign the variance with highest value to the visitor. The \n",
    "\n",
    "\n",
    "Thompson sampling – also known as posterior sampling and probability matching – was first proposed in 1933 (Thompson, 1933; Thompson,\n",
    "1935) for allocating experimental effort in two-armed bandit problems\n",
    "arising in clinical trials.\n",
    "\n",
    "\n",
    "## Thompson sampling algorithm\n",
    "<span style=\"color:blue\"># For a K-variants experiment, initialize with prior beta distributions:</span>   \n",
    "**for** *k = 1, ..., K* **do**    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\alpha_{k} = 1, \\beta_{k} = 1$  \n",
    "**end for**   \n",
    "<br>\n",
    "**for** *t = 1, 2, ...* **do**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style=\"color:blue\"># Sample form posterior beta distributions:</span>  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**for** *k = 1, ..., K* **do**    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sample&nbsp;&nbsp;$\\hat{\\theta}_{k} \\sim beta(\\alpha_{k}, \\beta_{k})$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**end for**   \n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style=\"color:blue\"># Select and apply action:</span>  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$x_{t}$ $\\leftarrow$ argmax$_{k} \\hat{\\theta}_{k}$       \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Apply action $x_{t}$ and observe reward $r_{t}$  \n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style=\"color:blue\"># Update the posterior beta distribution:</span>   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$(\\alpha_{x_{t}},\\beta_{x_{t}}) \\leftarrow (\\alpha_{x_{t}} + r_{t}, \\beta_{x_{t}} + 1 - r_{t})$  \n",
    "**end for**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimal probability; once 99% unlikely to move away if the rank doesn't change\n",
    "Credible interval; underestimated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "[1] O. Chapelle and L. Li. An empirical evaluation of Thompson sampling. \n",
    "      In NIPS, 2011."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
