{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-armed bandit (MAB) experiments aim to maximize performance of the primary metric across all variants. They achieve this by acquiring information about each variant (exploration) while simultaneously using updated information to optimize immediate rewards (exploitation). The latter is done by moving traffic gradually towards variants that are performing well while allocating less traffic to variants that are underperforming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thompson Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many MAB algorithms. Examples are Upper Confidence Bound, Greedy and Thompson Sampling. According to research studies[1], Thompson Sampling is one of the best MAB algorithms in terms of reward maximization. It is currently employed in One XP for MAB experiments. Thompson sampling uses the Bayesian approach. For each incoming user, it draws random samples from posterior distributions and assign the variant with the highest value to the user. The algorithm is described in detail as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><span style=\"color:blue\"># For a K-variants experiment, initialize with prior beta distributions:</span>   \n",
    ">**for** *k = 1, ..., K* **do**    \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\alpha_{k} = 1, \\beta_{k} = 1$  \n",
    ">**end for**   \n",
    "><br>\n",
    ">**for** *t = 1, 2, ...* **do**  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style=\"color:blue\"># Sample form  posterior beta distributions:</span>  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**for** *k = 1, ..., K* **do**    \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sample&nbsp;&nbsp;$\\hat{\\theta}_{k} \\sim beta(\\alpha_{k}, \\beta_{k})$  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**end for**   \n",
    "><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style=\"color:blue\"># Select and apply action:</span>  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$x_{t}$ $\\leftarrow$ argmax$_{k} \\hat{\\theta}_{k}$       \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Apply action $x_{t}$ and observe reward $r_{t}$  \n",
    "><br>\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style=\"color:blue\"># Update the posterior beta distribution:</span>   \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$(\\alpha_{x_{t}},\\beta_{x_{t}}) \\leftarrow (\\alpha_{x_{t}} + r_{t}, \\beta_{x_{t}} + 1 - r_{t})$  \n",
    ">**end for**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the production environment, rewards normally are not observed immediately after actions are applied. In order to account for such delayed feedback, batch update of posterior beta distribution is used. Currently the update frequency is set to hourly. The simulation study shows that Thompson Sampling with batch update performs equally well in the long run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] O. Chapelle and L. Li. An empirical evaluation of Thompson sampling. \n",
    "      In NIPS, 2011."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
